ccopy_reg
_reconstructor
p1
(c__main__
LogisticRegression
p2
c__builtin__
object
p3
NtRp4
(dp5
S'b'
g1
(ctheano.tensor.sharedvar
TensorSharedVariable
p6
g3
NtRp7
(dp8
S'auto_name'
p9
S'auto_22'
p10
sS'index'
p11
NsS'tag'
p12
g1
(ctheano.gof.utils
scratchpad
p13
g3
NtRp14
(dp15
S'trace'
p16
(lp17
(lp18
(S'full.py'
p19
I761
S'<module>'
p20
S"weights, costs = sgd_optimization_mnist('final')"
tp21
a(S'full.py'
p22
I317
S'sgd_optimization_mnist'
p23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp24
a(S'full.py'
p25
I30
S'__init__'
p26
S"), name='b', borrow=True)"
tp27
aasbsS'container'
p28
g1
(ctheano.gof.link
Container
p29
g3
NtRp30
(dp31
S'name'
p32
S'b'
sS'storage'
p33
(lp34
cnumpy.core.multiarray
_reconstruct
p35
(cnumpy
ndarray
p36
(I0
tS'b'
tRp37
(I1
(I3
tcnumpy
dtype
p38
(S'f8'
I0
I1
tRp39
(I3
S'<'
NNNI-1
I-1
I0
tbI00
S'\xd8m\x96:\x16\xec\xe0\xbf\xa6\x0c\xb3\xf6\x1f(\xe6\xbf?\xbd\xa4\x18\x1b\x8a\xf3?'
tbasS'strict'
p40
I00
sS'readonly'
p41
I00
sS'type'
p42
g1
(ctheano.tensor.type
TensorType
p43
g3
NtRp44
(dp45
S'broadcastable'
p46
(I00
tp47
sS'dtype'
p48
S'float64'
p49
sS'numpy_dtype'
p50
g39
sS'sparse_grad'
p51
I00
sg32
NsbsS'allow_downcast'
p52
Nsbsg32
S'b'
sS'owner'
p53
Nsg42
g44
sbsS'y_pred'
p54
g1
(ctheano.tensor.var
TensorVariable
p55
g3
NtRp56
(dp57
g9
S'auto_31'
p58
sg11
I1
sg12
g1
(g13
g3
NtRp59
(dp60
g16
(lp61
(lp62
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp63
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp64
a(g25
I34
g26
S'self.y_pred = T.argmax(self.p_y_given_x, axis=1)'
tp65
aasbsg32
S'argmax'
p66
sg53
g1
(ctheano.gof.graph
Apply
p67
g3
NtRp68
(dp69
S'inputs'
p70
(lp71
g1
(g55
g3
NtRp72
(dp73
g9
S'auto_29'
p74
sg11
I0
sg12
g1
(g13
g3
NtRp75
(dp76
g16
(lp77
(lp78
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp79
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp80
a(g25
I32
g26
S'self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)'
tp81
aasbsg32
Nsg53
g1
(g67
g3
NtRp82
(dp83
g70
(lp84
g1
(g55
g3
NtRp85
(dp86
g9
S'auto_28'
p87
sg11
I0
sg12
g1
(g13
g3
NtRp88
(dp89
g16
(lp90
(lp91
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp92
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp93
a(g25
I32
g26
S'self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)'
tp94
aasbsg32
Nsg53
g1
(g67
g3
NtRp95
(dp96
g70
(lp97
g1
(g55
g3
NtRp98
(dp99
g9
S'auto_23'
p100
sg11
I0
sg12
g1
(g13
g3
NtRp101
(dp102
g16
(lp103
(lp104
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp105
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp106
a(g25
I32
g26
S'self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)'
tp107
aasbsg32
Nsg53
g1
(g67
g3
NtRp108
(dp109
g70
(lp110
g1
(g55
g3
NtRp111
(dp112
g9
S'auto_19'
p113
sg11
Nsg12
g1
(g13
g3
NtRp114
(dp115
g16
(lp116
(lp117
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp118
a(g22
I311
g23
S"x = T.matrix('x')  # data, presented as rasterized images"
tp119
aasbsg32
S'x'
sg53
Nsg42
g1
(g43
g3
NtRp120
(dp121
g46
(I00
I00
tp122
sg48
S'float64'
p123
sg50
g39
sg51
I00
sg32
Nsbsbag1
(g6
g3
NtRp124
(dp125
g9
S'auto_21'
p126
sg11
Nsg12
g1
(g13
g3
NtRp127
(dp128
g16
(lp129
(lp130
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp131
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp132
a(g25
I23
g26
S"), name='W', borrow=True)"
tp133
aasbsg28
g1
(g29
g3
NtRp134
(dp135
g32
S'W'
sg33
(lp136
g35
(g36
(I0
tS'b'
tRp137
(I1
(I20
I3
tg39
I00
S"\xd9\x1e\x9dt\x87\xc0\xc0\xbfBl% w\xd1\xc1\xbf\x84EaJ\xffH\xd1?V\xc5\xcd\x8a\xf4\xc1\xb2\xbf,\x8c\xc7\xaad\x1d\xa1\xbfw\x8b1\xe0\xa6P\xbb?:\x8e\x12\xcd\xfa\xcc\xc9?\x02\xf73\x8e\xa1{\xcd?\x8fB\xa3-N\xa4\xdb\xbf\xa6YA\xf0\xbf\xf9\xa6\xbf\x01\xa4\xa2\xf4><\xa9\xbf\xe7\xfeqr\xff\x1a\xb8?\x16\x87Q\x9d\xe2u\xbf?Q,X\xdfC\xbc\xc2?\xdew\x00\x97\x9a;\xd1\xbf\xd7B\x8a\x05\x02\x0f\xab?n{w\x9c{\xdd\xb9?EN\x9eO~\xb2\xc3\xbf\xd7B\x8a\x05\x02\x0f\xab\xbfn{w\x9c{\xdd\xb9\xbfEN\x9eO~\xb2\xc3?\xe26`+\x1cf\xee\xbf\xeb\xb6_H\xa7\xd1\xd9\xbf?\t\xc8\xe7w\xa7\xf5?\x9a\x13]v\x82\x06\xd8?C\x00\xb7\x898\x92\xd2?\n\n\n\x80]L\xe5\xbf\x82\x0b\xfb\xe7\x91\x8b\xc8\xbf\x8c)\xea\xb4F\x06\xc4\xbf\x8e\x9arN\xecH\xd6?\x13\xa1\xd8S\xdc\x02\xb9\xbf\xfd\xaaT6:\x05\xa0\xbfB{\x81\xb7\xbc\x82\xc0?\x92K;\xdf\xc91\xe3?\xb7F\xddA\xbc\xfc\xc7\xbf\xd9\xf3\x87\x9d5e\xda\xbf\x19\x17\xaa;x6\xe4\xbf\xcfh\x99\xcf\x01\x15\xf5\xbf0tn\xed=0\xff?w\x13\xe3\xea\xc3\x84\xf7\xbf\x9d\xb9\xaa2\x95+\xe6?\x86m\x1b\xa3\xf2\xdd\xe8?|6\xcde\xaa\xf7\xf4?\x8a\xb2\xb2\x7f\xb5\xc7\xef\xbf\xe7t\xcf\x97>O\xd4\xbf\xc1;/\x8d@\xd1\xc0\xbf\xf8(\xc8@;\xae\xb8\xbf@P\x93-^(\xcd?\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x006M'3\xc3\x98\xc0\xbfzM\x19\x07\x7fx\xdc\xbf\x1azVPpb\xe2?6M'3\xc3\x98\xc0?zM\x19\x07\x7fx\xdc?\x1azVPpb\xe2\xbf\x94\xf4A\x0c\xca!\xf1?\x97\xbb\xd1\xd367h?_\xdd\xab\xa7\xe5-\xf1\xbf"
tbasg40
I00
sg41
I00
sg42
g1
(g43
g3
NtRp138
(dp139
g46
(I00
I00
tp140
sg48
S'float64'
p141
sg50
g39
sg51
I00
sg32
Nsbsg52
Nsbsg32
S'W'
sg53
Nsg42
g138
sbasg12
g1
(g13
g3
NtRp142
sS'outputs'
p143
(lp144
g98
asS'op'
p145
g1
(ctheano.tensor.basic
Dot
p146
g3
NtRp147
sbsg42
g1
(g43
g3
NtRp148
(dp149
g46
(I00
I00
tp150
sg48
S'float64'
p151
sg50
g39
sg51
I00
sg32
Nsbsbag1
(g55
g3
NtRp152
(dp153
g9
S'auto_27'
p154
sg11
I0
sg12
g1
(g13
g3
NtRp155
(dp156
g16
(lp157
(lp158
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp159
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp160
a(g25
I32
g26
S'self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)'
tp161
aasbsg32
Nsg53
g1
(g67
g3
NtRp162
(dp163
g70
(lp164
g7
asg12
g1
(g13
g3
NtRp165
sg143
(lp166
g152
asg145
g1
(ctheano.tensor.elemwise
DimShuffle
p167
g3
NtRp168
(dp169
S'input_broadcastable'
p170
g47
sS'shuffle'
p171
(lp172
I0
asS'augment'
p173
(lp174
I0
asS'drop'
p175
(lp176
sS'inplace'
p177
I01
sS'new_order'
p178
(S'x'
I0
tp179
sS'view_map'
p180
(dp181
I0
(lp182
I0
assbsbsg42
g1
(g43
g3
NtRp183
(dp184
g46
(I01
I00
tp185
sg48
g49
sg50
g39
sg51
I00
sg32
Nsbsbasg12
g1
(g13
g3
NtRp186
sg143
(lp187
g85
asg145
g1
(ctheano.tensor.elemwise
Elemwise
p188
g3
NtRp189
(dp190
S'__module__'
p191
S'tensor'
p192
sS'scalar_op'
p193
g1
(ctheano.scalar.basic
Add
p194
g3
NtRp195
(dp196
S'output_types_preference'
p197
ctheano.scalar.basic
upcast_out
p198
sg32
S'add'
p199
sbsg32
S'Elemwise{add,no_inplace}'
p200
sS'destroy_map'
p201
(dp202
sS'nfunc_spec'
p203
(g199
I2
I1
tp204
sS'inplace_pattern'
p205
g1
(ctheano.misc.frozendict
frozendict
p206
g3
NtRp207
(dp208
S'_hash'
p209
I0
sS'_dict'
p210
(dp211
sbsS'openmp'
p212
I00
sS'__doc__'
p213
S"elementwise addition\n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Note\n    ----\n    | Elemwise(add) represents + on tensors (x + y)\n    | Elemwise(add, {0 : 0}) represents the += operation (x += y)\n    | Elemwise(add, {0 : 1}) represents += on the second argument (y += x)\n    | Elemwise(mul)(rand(10, 5), rand(1, 5)) the second input is completed along the first dimension to match the first input\n    | Elemwise(true_div)(rand(10, 5), rand(10, 1)) same but along the second dimension\n    | Elemwise(int_div)(rand(1, 5), rand(10, 1)) the output has size (10, 5)\n    | Elemwise(log)(rand(3, 4, 5))\n\n    "
p214
sbsbsg42
g1
(g43
g3
NtRp215
(dp216
g46
(I00
I00
tp217
sg48
g123
sg50
g39
sg51
I00
sg32
Nsbsbasg12
g1
(g13
g3
NtRp218
sg143
(lp219
g72
asg145
g1
(ctheano.tensor.nnet.nnet
Softmax
p220
g3
NtRp221
sbsg42
g215
sbasg12
g1
(g13
g3
NtRp222
sg143
(lp223
g1
(g55
g3
NtRp224
(dp225
g9
S'auto_30'
p226
sg11
I0
sg12
g1
(g13
g3
NtRp227
(dp228
g16
(lp229
(lp230
(g19
I761
g20
S"weights, costs = sgd_optimization_mnist('final')"
tp231
a(g22
I317
g23
S'classifier = LogisticRegression(input=x, n_in=20*1, n_out=3)'
tp232
a(g25
I34
g26
S'self.y_pred = T.argmax(self.p_y_given_x, axis=1)'
tp233
aasbsg32
S'max'
p234
sg53
g68
sg42
g1
(g43
g3
NtRp235
(dp236
g46
(I00
tp237
sg48
g123
sg50
g39
sg51
I00
sg32
Nsbsbag56
asg145
g1
(ctheano.tensor.basic
MaxAndArgmax
p238
g3
NtRp239
(dp240
S'axis'
p241
(I1
tp242
sbsbsg42
g1
(g43
g3
NtRp243
(dp244
g46
(I00
tp245
sg48
S'int64'
p246
sg50
g38
(S'i8'
I0
I1
tRp247
(I3
S'<'
NNNI-1
I-1
I0
tbsg51
I00
sg32
NsbsbsS'params'
p248
(lp249
g124
ag7
asS'W'
g124
sS'input'
p250
g111
sS'p_y_given_x'
p251
g72
sb.